pdf("../Results//PP_Regress_Fig.pdf", 9, 11.7)
# Generating empty pdf file for figure
p <- qplot(Prey.mass,
Predator.mass,
data = MyDF,
log = "xy", # Logs data and axes
xlab = "Prey Mass in grams",
ylab = "Predator Mass in grams",
colour = Predator.lifestage,
shape = I(3),
size = I(2),
facets = Type.of.feeding.interaction~.)
p + geom_smooth(method = "lm", fullrange = T) + theme(legend.position = "bottom")
dev.off()
# Calculating regression results
# Group and subset variables using ddply and pass to linear model
df2 <- dlply(MyDF,.(Type.of.feeding.interaction, Predator.lifestage, Location), function(x) lm(Predator.mass~Prey.mass, data = x))
# Extract linear model results
lmextract <- ldply(df2, function(x) {
intercept <-summary(x)$coefficients[1]
slope <-summary(x)$coefficients[2]
p_val <- summary(x)$coefficients[8]
r2 <- summary(x)$r.squared
data.frame(slope, intercept, r2, p_val)
})
#
f_stat <- ldply(df2, function(x) summary(x)$fstat[1])
#
final_table <- merge(lmextract, f_stat, by = c("Type.of.feeding.interaction",
"Predator.lifestage", "Location"), all = T)
#
names(final_table)[8] = "F statistc"
write.csv(final_table,"../Results/PP_Regress_Results_Loc.csv", row.names = F)
#!/usr/bin/env Rscript
# Author: Eva Linehan
# Date: October 2018
# Desc: Wrangling the Pound Hill Dataset using dplyr and tidyr
#clear environments
rm(list=ls())
############# Load the dataset and packages ###############
# header = false because the raw data don't have real headers
MyData <- as.matrix(read.csv("../Data/PoundHillData.csv",header = F))
# header = true because we do have metadata headers
MyMetaData <- read.csv("../Data/PoundHillMetaData.csv",header = T, sep=";", stringsAsFactors = F)
library(tidyr)
library(dplyr)
############# Inspect the dataset ###############
head(MyData)
dplyr::glimpse(MyData)
dplyr::glimpse(MyMetaData)
utils::View(MyData)
############# Transpose ###############
# To get those species into columns and treatments into rows
MyData <- t(MyData) # Swaps columns and rows
head(MyData)
############# Replace species absences with zeros ###############
MyData[MyData == ""] = 0
############# Convert raw matrix to data frame ###############
TempData <- as.data.frame(MyData[-1,],stringsAsFactors = F) #stringsAsFactors = F is important!
colnames(TempData) <- MyData[1,] # assign column names from original data
############# Convert from wide to long format  ###############
MyWrangledData<- TempData %>% gather(.,Species, Count, -Cultivation, -Block, -Plot, -Quadrat) %>% mutate(Cultivation = as.factor(Cultivation), Block=as.factor(Block), Plot = as.factor(Plot), Quadrat = as.factor(Quadrat))
utils::View(MyWrangledData)
############# Start exploring the data (extend the script below)!  ###############
#!/usr/bin/env Rscript
# Author: Eva Linehan
# Date: October 2018
# Desc: This function calculates heights of trees given distance of each tree
# from its base and angle to its top, using  the trigonometric formula
#clear environments
rm(list=ls())
# height = distance * tan(radians)
#
# ARGUMENTS
# degrees:   The angle of elevation of tree
# distance:  The distance from base of tree (e.g., meters)
#
# OUTPUT
# The heights of the tree, same units as "distance"
Data<-read.csv("../Data/trees.csv", header = TRUE)
Data<- read.table("../Data/trees.csv", sep = ",", header = TRUE)
TreeHeight <- function(degrees,distance) {
radians<-degrees*pi/180
height<-distance*tan(radians)
}
Tree.Height.m<-TreeHeight(Data$Angle.degrees,Data$Distance.m)
OutputData<- cbind(Data, Tree.Height.m)
write.csv(OutputData, "../Results/TreeHts.csv", row.names=TRUE) # write row names
# This script calculates heights of trees from a given .csv file and outputs
# the result in the following format; "InputFileName_treeheights.csv"
#
# The height is calculated bu using the given distance of each tree from its base
# and angle to its top, using  the trigonometric formula;
#
# height = distance * tan(radians)
#
directory <- "../Data/"
filenames <- list.files(directory, pattern = "*.csv", full.names = TRUE)
Data<-read.csv(filenames, header = TRUE)
Data<- read.table(filenames, sep = ",", header = TRUE)
TreeHeight <- function(degrees,distance) {
radians<-degrees*pi/180
height<-distance*tan(radians)
print(paste("Tree height is:",height))
return (height)
}
Height.m<-TreeHeight(Data$Angle.degrees,Data$Distance.m)
OutputData<- cbind(Data, Height.m)
write.csv(OutputData, "../Results/TreeHts2.csv", row.names=TRUE) # write row names
#!/usr/bin/env Rscript
# Author: Eva Linehan
# Date: October 2018
# Desc: Script to plot Girko's Law simulation and save as pdf.
#clear environments
rm(list=ls())
build_ellipse <- function(hradius, vradius){
npoints = 250
a <- seq(0, 2 * pi, length = npoints + 1)
x <- hradius * cos(a)
y <- vradius * sin(a)
return(data.frame(x = x, y = y))}
require(ggplot2)
N <- 250 # Size of the matrix
M <- matrix(rnorm(N * N), N, N) # Build the matrix
eigvals <- eigen(M)$values
eigDF <- data.frame("Real" = Re(eigvals), "Imaginary" = Im(eigvals)) # Build a dataframe
my_radius <- sqrt(N) # The radius of the circle is sqrt(N)
ellDF <- build_ellipse(my_radius, my_radius) # Dataframe to plot the ellipse
names(ellDF) <- c("Real", "Imaginary") # rename the columns
p <- ggplot(eigDF, aes(x = Real, y = Imaginary))
p <- p +
geom_point(shape = I(3)) +
theme(legend.position = "none")
# Add the vertical and horizontal line
p <- p + geom_hline(aes(yintercept = 0))
p <- p + geom_vline(aes(xintercept = 0))
# Add ellipse and save to pdf
p <- p + geom_polygon(data = ellDF, aes(x = Real, y = Imaginary, alpha = 1/20, fill = "red"))
pdf("../Results/Girko.pdf",
11.7, 8.3)
p
dev.off()
#!/usr/bin/env Rscript
# Author: Eva Linehan
# Date: October 2018
# Desc: Annotating plots using geom text.
#clear environments
rm(list=ls())
dev.off()
a<-read.table("../Data/Results.txt", header = T)
head(a)
a$ymin<- rep(0, dim(a)[1]) # append column of 0s
# Print first linerange
p <- ggplot(a)
p <- p + geom_linerange(data = a, aes(
x = x,
ymin = ymin,
ymax = y1,
size = (0.5)
),
colour = "#E69F00",
alpha = 1/2, show.legend = FALSE)
p
# Second linerange
p <- p + geom_linerange(data = a, aes(
x = x,
ymin = ymin,
ymax = y2,
size = (0.5)
),
colour = "#56B4E9",
alpha = 1/2, show.legend = FALSE)
p
# Print third linerange
p <- p + geom_linerange(data = a, aes(
x = x,
ymin = ymin,
ymax = y3,
size = (0.5)
),
colour = "#D55E00",
alpha = 1/2, show.legend = FALSE)
p
# Annotate plot with labels
p <- p + geom_text(data = a, aes(x = x, y = -500, label = Label))
p
# Axis labels, remove legend, prepare for bw print
p <- p + scale_x_continuous("My x axis",
breaks = seq(3, 5, by = 0.05)) +
scale_y_continuous("My y axis") +
theme_bw() +
theme(legend.position = "none")
# Saving plot as .pdf file
pdf("../Results/MyBars.pdf",
11.7, 8.3)
p
dev.off()
#!/usr/bin/env Rscript
# Author: Eva Linehan
# Date: October 2018
# Desc: Script to demonstrate mathematical annotation on an axis
#clear environments
rm(list=ls())
# Create linear regression data
x<- seq(0, 100, by= 0.1)
y<- -4. + 0.25*x +
rnorm(length(x), mean = 0., sd = 2.5)
# and put them in a dataframe
my_data <- data.frame( x=x, y=y)
my_data
# perform a linear regression
my_lm <- summary(lm(y ~ x, data = my_data))
# plot the data
p <-  ggplot(my_data, aes(x = x, y = y,
colour = abs(my_lm$residual))
) +
geom_point() +
scale_colour_gradient(low = "black", high = "red") +
theme(legend.position = "none") +
scale_x_continuous(
expression(alpha^2 * pi / beta * sqrt(Theta)))
# add the regression line
p <- p + geom_abline(
intercept = my_lm$coefficients[1][1],
slope = my_lm$coefficients[2][1],
colour = "red")
# throw some math on the plot
p <- p + geom_text(aes(x = 60, y = 0,
label = "sqrt(alpha) * 2* pi"),
parse = TRUE, size = 6,
colour = "blue")
# Save to pdf
pdf("../Results/MyLinReg.pdf",
11.7, 8.3)
p
dev.off()
#!/usr/bin/env Rscript
# Author: Eva Linehan
# Date: October 2018
# Desc: Script to generate plot of changes in predator mass with prey mass across different
# types of feeding interactions and grouped by predator lifestage.
#clear environments
rm(list=ls())
require(ggplot2)
require(plyr)
require(dplyr)
MyDF<-read.csv("../Data/EcolArchives-E089-51-D1.csv")
pp <- MyDF %>% rowwise() %>%  mutate(Prey.mass = ifelse(Prey.mass.unit == "mg", Prey.mass/1000, Prey.mass))
pdf("../Results//PP_Regress_Fig.pdf", 9, 11.7)
# Generating empty pdf file for figure
p <- qplot(Prey.mass,
Predator.mass,
data = MyDF,
log = "xy", # Logs data and axes
xlab = "Prey Mass in grams",
ylab = "Predator Mass in grams",
colour = Predator.lifestage,
shape = I(3),
size = I(2),
facets = Type.of.feeding.interaction~.)
p + geom_smooth(method = "lm", fullrange = T) + theme(legend.position = "bottom")
dev.off()
# Calculating regression results
# Group and subset variables using ddply and pass to linear model
df2 <- dlply(MyDF,.(Type.of.feeding.interaction, Predator.lifestage), function(x) lm(Predator.mass~Prey.mass, data = x))
# Extract linear model results and make a data frame of the coefficients
lmextract <- ldply(df2, function(x) {
intercept <-summary(x)$coefficients[1]
slope <-summary(x)$coefficients[2]
p_val <- summary(x)$coefficients[8]
r2 <- summary(x)$r.squared
data.frame(slope, intercept, r2, p_val)
})
# Add the f stat seperately using the same ldply function
f_stat <- ldply(df2, function(x) summary(x)$fstat[1])
# Merge f stat and other coefficients into the final dataframe
final_table <- merge(lmextract, f_stat, by = c("Type.of.feeding.interaction",
"Predator.lifestage"), all = T)
# Give F stat column a header
names(final_table)[7] = "F statistc"
write.csv(final_table,"../Results/PP_Regress_Results.csv", row.names = F)
# Write to a csv file
pdf("../Results//PP_Regress_Fig.pdf", 8, 11.7)
p <- qplot(Prey.mass,
Predator.mass,
data = MyDF,
log = "xy", # Logs data and axes
xlab = "Prey Mass in grams",
ylab = "Predator Mass in grams",
colour = Predator.lifestage,
shape = I(3),
size = I(2),
facets = Type.of.feeding.interaction~.)
p + geom_smooth(method = "lm", fullrange = T) + theme(legend.position = "bottom")
#clear environments
rm(list=ls())
require(ggplot2)
require(ggplot2)
MyDF<-read.csv("../Data/EcolArchives-E089-51-D1.csv")
pp <- MyDF %>% rowwise() %>%  mutate(Prey.mass = ifelse(Prey.mass.unit == "mg", Prey.mass/1000, Prey.mass))
pdf("../Results//PP_Regress_Fig.pdf", 8, 11.7)
p <- qplot(Prey.mass,
Predator.mass,
data = MyDF,
log = "xy", # Logs data and axes
xlab = "Prey Mass in grams",
ylab = "Predator Mass in grams",
colour = Predator.lifestage,
shape = I(3),
size = I(2),
facets = Type.of.feeding.interaction~.)
p + geom_smooth(method = "lm", fullrange = T) + theme(legend.position = "bottom")
dev.off()
pdf("../Results//PP_Regress_Fig.pdf", 7, 11.7)
p <- qplot(Prey.mass,
Predator.mass,
data = MyDF,
log = "xy", # Logs data and axes
xlab = "Prey Mass in grams",
ylab = "Predator Mass in grams",
colour = Predator.lifestage,
shape = I(3),
size = I(2),
facets = Type.of.feeding.interaction~.)
p + geom_smooth(method = "lm", fullrange = T) + theme(legend.position = "bottom")
dev.off()
#!/usr/bin/env Rscript
# Author: Eva Linehan
# Date: October 2018
# Desc: Autocorrelation in weather
### Calculating the correlation of mean temperatures in Key West, Florida for successive years in the 20th century
#clear environments
rm(list=ls())
# Analyze our data
load("../Data/KeyWestAnnualMeanTemperature.RData")
head(ats) # Display first 6 rows of data
Key<-ats
scatter.smooth(Key) # Scatter plot with line of best fit
summary(Key$Temp)
Temp<-Key$Temp
Year<-Key$Year
# Align temperature data between successive years to compare
t1<-Key[1:99,2]
t2<-Key[2:100,2]
Corr_successive_years<-cor(t1,t2)
Corr_successive_years
# Repeat function
tf<-function(t1,t2){
t1<-sample(t1,size = length(t1),replace = TRUE)
t2<-sample(t2,size = length(t2),replace=TRUE)
return(cor(t1,t2))
}
# Randomize the alignment of t1 and t2 subsets, correlate these random alignments
result<-sapply(1:10000, function(i) tf(t1,t2))
# Run the t1 and t2 subsets through this function 10000 times
output<-result>Corr_successive_years
# Assign output variable to the number of cases in which randomly
# permuted time series correlations were greater than that of the correlation
# between successive years.
Fraction<-1-(length(output[output==FALSE])/length(result))
# The fraction of random correlations greater than that of successive years
# Length of cases in which random correlations were greater than successive
# years compared to length of all random correlations minus 1
Fraction #print
#Few correlations greater than ours, weak correlation but significant beacuse
#compared to a distribution of randomized permeated etc.
# our value explains it better, our value is different from random
# Interpret results
library(ggplot2)
hist(result,main=NULL,xlab = "Correlation_Result")
abline(v=Corr_successive_years, col="blue")
#!/usr/bin/env Rscript
# Author: Eva Linehan
# Date: October 2018
# Desc: Autocorrelation in weather
### Calculating the correlation of mean temperatures in Key West, Florida for successive years in the 20th century
#clear environments
rm(list=ls())
# Analyze our data
load("../Data/KeyWestAnnualMeanTemperature.RData")
head(ats) # Display first 6 rows of data
Key<-ats
scatter.smooth(Key) # Scatter plot with line of best fit
summary(Key$Temp)
Temp<-Key$Temp
Year<-Key$Year
# Align temperature data between successive years to compare
t1<-Key[1:99,2]
t2<-Key[2:100,2]
Corr_successive_years<-cor(t1,t2)
Corr_successive_years
# Repeat function
tf<-function(t1,t2){
t1<-sample(t1,size = length(t1),replace = TRUE)
t2<-sample(t2,size = length(t2),replace=TRUE)
return(cor(t1,t2))
}
# Randomize the alignment of t1 and t2 subsets, correlate these random alignments
result<-sapply(1:10000, function(i) tf(t1,t2))
# Run the t1 and t2 subsets through this function 10000 times
output<-result>Corr_successive_years
# Assign output variable to the number of cases in which randomly
# permuted time series correlations were greater than that of the correlation
# between successive years.
Fraction<-1-(length(output[output==FALSE])/length(result))
# The fraction of random correlations greater than that of successive years
# Length of cases in which random correlations were greater than successive
# years compared to length of all random correlations minus 1
Fraction #print
#Few correlations greater than ours, weak correlation but significant beacuse
#compared to a distribution of randomized permeated etc.
# our value explains it better, our value is different from random
# Interpret results
library(ggplot2)
pdf("../Results/TAutoCorr_hist.pdf", 9, 11.7)
hist(result,main=NULL,xlab = "Correlation_Result")
abline(v=Corr_successive_years, col="blue")
dev.off()
#!/usr/bin/env Rscript
# Author: Eva Linehan
# Date: October 2018
# Desc: Autocorrelation in weather
### Calculating the correlation of mean temperatures in Key West, Florida for successive years in the 20th century
#clear environments
rm(list=ls())
# Analyze our data
load("../Data/KeyWestAnnualMeanTemperature.RData")
head(ats) # Display first 6 rows of data
Key<-ats
scatter.smooth(Key) # Scatter plot with line of best fit
summary(Key$Temp)
Temp<-Key$Temp
Year<-Key$Year
# Align temperature data between successive years to compare
t1<-Key[1:99,2]
t2<-Key[2:100,2]
Corr_successive_years<-cor(t1,t2)
Corr_successive_years
# Repeat function
tf<-function(t1,t2){
t1<-sample(t1,size = length(t1),replace = TRUE)
t2<-sample(t2,size = length(t2),replace=TRUE)
return(cor(t1,t2))
}
# Randomize the alignment of t1 and t2 subsets, correlate these random alignments
result<-sapply(1:10000, function(i) tf(t1,t2))
# Run the t1 and t2 subsets through this function 10000 times
output<-result>Corr_successive_years
# Assign output variable to the number of cases in which randomly
# permuted time series correlations were greater than that of the correlation
# between successive years.
Fraction<-1-(length(output[output==FALSE])/length(result))
# The fraction of random correlations greater than that of successive years
# Length of cases in which random correlations were greater than successive
# years compared to length of all random correlations minus 1
Fraction #print
#Few correlations greater than ours, weak correlation but significant beacuse
#compared to a distribution of randomized permeated etc.
# our value explains it better, our value is different from random
# Interpret results
library(ggplot2)
pdf("../Data/TAutoCorr_hist.pdf", 9, 11.7)
hist(result,main=NULL,xlab = "Correlation_Result")
abline(v=Corr_successive_years, col="blue")
dev.off()
#!/usr/bin/env Rscript
# Author: Eva Linehan
# Date: October 2018
# Desc: Autocorrelation in weather
### Calculating the correlation of mean temperatures in Key West, Florida for successive years in the 20th century
#clear environments
rm(list=ls())
# Analyze our data
load("../Data/KeyWestAnnualMeanTemperature.RData")
head(ats) # Display first 6 rows of data
Key<-ats
scatter.smooth(Key) # Scatter plot with line of best fit
summary(Key$Temp)
Temp<-Key$Temp
Year<-Key$Year
# Align temperature data between successive years to compare
t1<-Key[1:99,2]
t2<-Key[2:100,2]
Corr_successive_years<-cor(t1,t2)
Corr_successive_years
# Repeat function
tf<-function(t1,t2){
t1<-sample(t1,size = length(t1),replace = TRUE)
t2<-sample(t2,size = length(t2),replace=TRUE)
return(cor(t1,t2))
}
# Randomize the alignment of t1 and t2 subsets, correlate these random alignments
result<-sapply(1:10000, function(i) tf(t1,t2))
# Run the t1 and t2 subsets through this function 10000 times
output<-result>Corr_successive_years
# Assign output variable to the number of cases in which randomly
# permuted time series correlations were greater than that of the correlation
# between successive years.
Fraction<-1-(length(output[output==FALSE])/length(result))
# The fraction of random correlations greater than that of successive years
# Length of cases in which random correlations were greater than successive
# years compared to length of all random correlations minus 1
Fraction #print
#Few correlations greater than ours, weak correlation but significant beacuse
#compared to a distribution of randomized permeated etc.
# our value explains it better, our value is different from random
# Interpret results
library(ggplot2)
pdf("../Data/TAutoCorr_hist.pdf", p)
hist(result,main=NULL,xlab = "Correlation_Result")
abline(v=Corr_successive_years, col="blue")
dev.off()
